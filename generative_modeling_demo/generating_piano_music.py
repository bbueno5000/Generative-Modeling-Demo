# -*- coding: utf-8 -*-

"""
Generating Piano Music with Transformer

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebooks/magenta/piano_transformer/piano_transformer.ipynb

Copyright 2019 Google LLC.

Licensed under the Apache License, Version 2.0 (the "License");

Generating Piano Music with Transformer

___Ian Simon, Anna Huang, Jesse Engel, Curtis "Fjord" Hawthorne___

This Colab notebook lets you play with pretrained [Transformer](https://arxiv.org/abs/1706.03762) models for piano music generation, based on the [Music Transformer](http://g.co/magenta/music-transformer) model introduced by [Huang et al.](https://arxiv.org/abs/1809.04281) in 2018.

The models used here were trained on over 10,000 hours of piano recordings from YouTube, transcribed using [Onsets and Frames](http://g.co/magenta/onsets-frames) and represented using the event vocabulary from [Performance RNN](http://g.co/magenta/performance-rnn).

Unlike the original Music Transformer paper, this notebook uses attention based on absolute instead of relative position; we may add models that use relative attention at some point in the future.

Environment Setup
"""
import ctypes.util

def proxy_find_library(lib):
  if lib == 'fluidsynth':
    return 'libfluidsynth.so.1'
  else:
    return ctypes.util.find_library(lib)

ctypes.util.find_library = proxy_find_library

print('Importing libraries...')

import numpy as np
import os
import tensorflow.compat.v1 as tf
from google.colab import files
from tensor2tensor import models
from tensor2tensor import problems
from tensor2tensor.data_generators import text_encoder
from tensor2tensor.utils import decoding
from tensor2tensor.utils import trainer_lib
from magenta.models.score2perf import score2perf
import note_seq

tf.disable_v2_behavior()

print('Done!')

SF2_PATH = '/content/Yamaha-C5-Salamander-JNv5.1.sf2'
SAMPLE_RATE = 16000

def upload_midi():
    """
    Upload a MIDI file and convert to NoteSequence.
    """
    data = list(files.upload().values())
    if len(data) > 1:
        print('Multiple files uploaded; using only one.')
    return note_seq.midi_to_note_sequence(data[0])

def decode(ids, encoder):
    """
    Decode a list of IDs.
    """
    ids = list(ids)
    if text_encoder.EOS_ID in ids:
        ids = ids[:ids.index(text_encoder.EOS_ID)]
    return encoder.decode(ids)

model_name = 'transformer'
hparams_set = 'transformer_tpu'
ckpt_path = 'gs://magentadata/models/music_transformer/checkpoints/unconditional_model_16.ckpt'

class PianoPerformanceLanguageModelProblem(score2perf.Score2PerfProblem):
    """
    DOCSTRING
    """
    @property
    def add_eos_symbol(self):
        return True

problem = PianoPerformanceLanguageModelProblem()
unconditional_encoders = problem.get_feature_encoders()

# set up HParams
hparams = trainer_lib.create_hparams(hparams_set=hparams_set)
trainer_lib.add_problem_hparams(hparams, problem)
hparams.num_hidden_layers = 16
hparams.sampling_method = 'random'

# set up decoding HParams
decode_hparams = decoding.decode_hparams()
decode_hparams.alpha = 0.0
decode_hparams.beam_size = 1

# create Estimator
run_config = trainer_lib.create_run_config(hparams)
estimator = trainer_lib.create_estimator(
    model_name, hparams, run_config, decode_hparams=decode_hparams)

def input_generator():
    """
    Create input generator,
    so we can adjust priming and decode length on the fly.
    """
    global targets
    global decode_length
    while True:
        yield {
            'targets': np.array([targets], dtype=np.int32),
            'decode_length': np.array(decode_length, dtype=np.int32)
            }

targets = list()

decode_length = 0
input_fn = decoding.make_input_fn_from_generator(input_generator())
unconditional_samples = estimator.predict(input_fn, checkpoint_path=ckpt_path)

_ = next(unconditional_samples)

targets = list()
decode_length = 1024

sample_ids = next(unconditional_samples)['outputs']

midi_filename = decode(sample_ids, encoder=unconditional_encoders['targets'])
unconditional_ns = note_seq.midi_file_to_note_sequence(midi_filename)

note_seq.play_sequence(
    unconditional_ns,
    synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)
note_seq.plot_sequence(unconditional_ns)

note_seq.sequence_proto_to_midi_file(unconditional_ns, '/tmp/unconditional.mid')
files.download('/tmp/unconditional.mid')

filenames = {
    'C major arpeggio': '/content/c_major_arpeggio.mid',
    'C major scale': '/content/c_major_scale.mid',
    'Clair de Lune': '/content/clair_de_lune.mid',
}

primer = 'C major scale'

if primer == 'Upload your own!':
    primer_ns = upload_midi()
else:
    primer_ns = note_seq.midi_file_to_note_sequence(filenames[primer])

primer_ns = note_seq.apply_sustain_control_changes(primer_ns)

max_primer_seconds = 20

if primer_ns.total_time > max_primer_seconds:
    print('Primer is longer than %d seconds, truncating.' % max_primer_seconds)
    primer_ns = note_seq.extract_subsequence(primer_ns, 0, max_primer_seconds)

if any(note.is_drum for note in primer_ns.notes):
    print('Primer contains drums; they will be removed.')
    notes = [note for note in primer_ns.notes if not note.is_drum]
    del primer_ns.notes[:]
    primer_ns.notes.extend(notes)

for note in primer_ns.notes:
    note.instrument = 1
    note.program = 0

note_seq.play_sequence(
    primer_ns, synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)
note_seq.plot_sequence(primer_ns)

targets = unconditional_encoders['targets'].encode_note_sequence(primer_ns)

targets = targets[:-1]

decode_length = max(0, 4096 - len(targets))

if len(targets) >= 4096:
    print('Primer has more events than maximum sequence length; nothing will be generated.')

sample_ids = next(unconditional_samples)['outputs']

midi_filename = decode(sample_ids, encoder=unconditional_encoders['targets'])
ns = note_seq.midi_file_to_note_sequence(midi_filename)

continuation_ns = note_seq.concatenate_sequences([primer_ns, ns])

note_seq.play_sequence(
    continuation_ns,
    synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)
note_seq.plot_sequence(continuation_ns)

note_seq.sequence_proto_to_midi_file(continuation_ns, '/tmp/continuation.mid')
files.download('/tmp/continuation.mid')

model_name = 'transformer'
hparams_set = 'transformer_tpu'
ckpt_path = 'gs://magentadata/models/music_transformer/checkpoints/melody_conditioned_model_16.ckpt'

class MelodyToPianoPerformanceProblem(score2perf.AbsoluteMelody2PerfProblem):
    """
    DOCSTRING
    """
    @property
    def add_eos_symbol(self):
        return True

problem = MelodyToPianoPerformanceProblem()
melody_conditioned_encoders = problem.get_feature_encoders()

hparams = trainer_lib.create_hparams(hparams_set=hparams_set)
trainer_lib.add_problem_hparams(hparams, problem)
hparams.num_hidden_layers = 16
hparams.sampling_method = 'random'

decode_hparams = decoding.decode_hparams()
decode_hparams.alpha = 0.0
decode_hparams.beam_size = 1

run_config = trainer_lib.create_run_config(hparams)
estimator = trainer_lib.create_estimator(
    model_name, hparams, run_config,
    decode_hparams=decode_hparams)

inputs = list()
decode_length = 0

def input_generator():
  global inputs
  while True:
    yield {
        'inputs': np.array([[inputs]], dtype=np.int32),
        'targets': np.zeros([1, 0], dtype=np.int32),
        'decode_length': np.array(decode_length, dtype=np.int32)
    }

input_fn = decoding.make_input_fn_from_generator(input_generator())
melody_conditioned_samples = estimator.predict(input_fn, checkpoint_path=ckpt_path)

_ = next(melody_conditioned_samples)

event_padding = 2 * [note_seq.MELODY_NO_EVENT]

melodies = {
    'Mary Had a Little Lamb': [
        64, 62, 60, 62, 64, 64, 64, note_seq.MELODY_NO_EVENT,
        62, 62, 62, note_seq.MELODY_NO_EVENT,
        64, 67, 67, note_seq.MELODY_NO_EVENT,
        64, 62, 60, 62, 64, 64, 64, 64,
        62, 62, 64, 62, 60, note_seq.MELODY_NO_EVENT,
        note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,
    ],
    'Row Row Row Your Boat': [
        60, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,
        60, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,
        60, note_seq.MELODY_NO_EVENT, 62,
        64, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,
        64, note_seq.MELODY_NO_EVENT, 62,
        64, note_seq.MELODY_NO_EVENT, 65,
        67, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,
        note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,
        72, 72, 72, 67, 67, 67, 64, 64, 64, 60, 60, 60,
        67, note_seq.MELODY_NO_EVENT, 65,
        64, note_seq.MELODY_NO_EVENT, 62,
        60, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,
        note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,
    ],
    'Twinkle Twinkle Little Star': [
        60, 60, 67, 67, 69, 69, 67, note_seq.MELODY_NO_EVENT,
        65, 65, 64, 64, 62, 62, 60, note_seq.MELODY_NO_EVENT,
        67, 67, 65, 65, 64, 64, 62, note_seq.MELODY_NO_EVENT,
        67, 67, 65, 65, 64, 64, 62, note_seq.MELODY_NO_EVENT,
        60, 60, 67, 67, 69, 69, 67, note_seq.MELODY_NO_EVENT,
        65, 65, 64, 64, 62, 62, 60, note_seq.MELODY_NO_EVENT,
    ]
}

melody = 'Twinkle Twinkle Little Star'

if melody == 'Upload your own!':
    melody_ns = upload_midi()
    melody_instrument = note_seq.infer_melody_for_sequence(melody_ns)
    notes = [note for note in melody_ns.notes if note.instrument == melody_instrument]
    del melody_ns.notes[:]
    melody_ns.notes.extend(sorted(notes, key=lambda note: note.start_time))
    for i in range(len(melody_ns.notes) - 1):
        melody_ns.notes[i].end_time = melody_ns.notes[i + 1].start_time
    inputs = melody_conditioned_encoders['inputs'].encode_note_sequence(melody_ns)
else:
    events = [
        event + 12 if event != note_seq.MELODY_NO_EVENT else event
        for e in melodies[melody] for event in [e] + event_padding]
    inputs = melody_conditioned_encoders['inputs'].encode(
        ' '.join(str(e) for e in events))
    melody_ns = note_seq.Melody(events).to_sequence(qpm=150)

note_seq.play_sequence(
    melody_ns, synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)
note_seq.plot_sequence(melody_ns)

decode_length = 4096
sample_ids = next(melody_conditioned_samples)['outputs']

midi_filename = decode(
    sample_ids, encoder=melody_conditioned_encoders['targets'])
accompaniment_ns = note_seq.midi_file_to_note_sequence(midi_filename)

note_seq.play_sequence(
    accompaniment_ns, synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)
note_seq.plot_sequence(accompaniment_ns)

note_seq.sequence_proto_to_midi_file(accompaniment_ns, '/tmp/accompaniment.mid')
files.download('/tmp/accompaniment.mid')
